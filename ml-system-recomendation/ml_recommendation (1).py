# -*- coding: utf-8 -*-
"""ML- recommendation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1z43hv2V-l8QDdHDaFLVjwiU89SBOuSpt
"""

import os
import pandas as pd
import numpy as np
import tensorflow as tf

from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.layers import Dropout
from sklearn.metrics.pairwise import cosine_similarity

from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import MinMaxScaler
from sklearn.feature_extraction.text import TfidfVectorizer

from google.colab import drive

drive.mount('/content/gdrive')
os.environ['KAGGLE_CONFIG_DIR'] = "/content/gdrive/My Drive/Kaggle"

!kaggle datasets download -d CooperUnion/anime-recommendations-database

!unzip '/content/gdrive/MyDrive/Kaggle/anime-recommendations-database.zip'

dataset_anime = pd.read_csv('/content/gdrive/MyDrive/Kaggle/anime.csv')
dataset_rating = pd.read_csv('/content/gdrive/MyDrive/Kaggle/rating.csv')

dataset_anime.info()

print('Total anime dengan id yang unique: ', len(dataset_anime.anime_id.unique()))

dataset_anime.isna().sum()

dataset_rating.isna().sum()

print('Total anime yang sudah di rating: ', len(dataset_rating.anime_id.unique()))

dataset_rating.info()

dataset_anime.dropna(inplace=True)

isna_isnull_dataset_anime = pd.DataFrame({
    'isnull': dataset_anime.isnull().sum(),
    'isna':dataset_anime.isna().sum()
})
isna_isnull_dataset_anime

isna_isnull_dataset_rating = pd.DataFrame({
    'isnull': dataset_rating.isnull().sum(),
    'isna':dataset_rating.isna().sum()
})
isna_isnull_dataset_rating

"""COLLABORATIVE BASE"""

dataset_anime.rename(columns={'rating':'rating_all'})

dataset_combination = pd.merge(dataset_rating,dataset_anime[['anime_id','name']],how='left',on='anime_id')
dataset_combination = dataset_combination[dataset_combination['rating'] > 0]

dataset_combination = dataset_combination.drop_duplicates(['anime_id','user_id'])

"""- Mengubah user_id menjadi list tanpa nilai yang sama
- Melakukan encoding user_id
- Melakukan proses encoding angka ke ke user_id

"""

user_ids = dataset_combination['user_id'].unique().tolist()

user_to_user_encoded = {x: i for i, x in enumerate(user_ids)}

user_encoded_to_user = {i: x for i, x in enumerate(user_ids)}

"""- Mengubah anime_id menjadi list tanpa nilai yang sama
- Melakukan proses encoding angka ke anime_id
- Melakukan proses encoding anime_id
"""

anime_ids = dataset_combination['anime_id'].unique().tolist()

animeid_to_animeid_encoded = {x: i for i, x in enumerate(anime_ids)}

animeid_encoded_to_animeid = {i: x for i, x in enumerate(anime_ids)}

"""- Mendapatkan jumlah user
- Mendapatkan jumlah resto


"""

total_user = len(user_to_user_encoded)
print("total user : " , total_user)

total_anime = len(animeid_to_animeid_encoded)
print("total user : " , total_anime)

"""- Mapping user_id ke dataframe user_id_encoded
- Mapping anime_id ke dataframe anime_id_encoded
"""

dataset_combination['user_id_encoded'] = dataset_combination['user_id'].map(user_to_user_encoded)

dataset_combination['anime_id_encoded'] = dataset_combination['anime_id'].map(animeid_to_animeid_encoded)

"""- Mengubah rating menjadi nilai float"""

dataset_combination['rating'] = dataset_combination['rating'].values.astype(np.float32)

"""- Mengacak dataset"""

dataset_combination = dataset_combination.sample(random_state=42,n=5000)
dataset_combination

min_rating = min(dataset_combination.rating)
max_rating = max(dataset_combination.rating)

features = dataset_combination[['user_id_encoded','anime_id_encoded']].values
label = dataset_combination['rating'].values

x_train, x_test, y_train, y_test = train_test_split(
    features,
    label,
    test_size=0.2,
    random_state=42
)

scaler = MinMaxScaler()
y_train = scaler.fit_transform(y_train.reshape(-1,1))
y_test = scaler.transform(y_test.reshape(-1, 1))

class RecommenderNet(tf.keras.Model):

  def __init__(self, total_user, total_anime, embedding_size, **kwargs):

    super(RecommenderNet, self).__init__(**kwargs)

    self.total_user = total_user
    self.total_anime = total_anime
    self.embedding_size = embedding_size

    self.user_embedding = layers.Embedding(
        total_user,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(0.00005)
    )

    self.user_bias = layers.Embedding(total_user, 1)

    self.anime_embedding = layers.Embedding(
        total_anime,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(0.005)
    )
    self.anime_bias = layers.Embedding(total_anime, 1)


    self.dropout = Dropout(0.00005)

  def call(self, inputs):

    user_vector = self.user_embedding(inputs[:,0])
    user_bias = self.user_bias(inputs[:, 0])

    anime_vector = self.anime_embedding(inputs[:, 1])
    anime_bias = self.anime_bias(inputs[:, 1])

    dot_user_anime = tf.tensordot(user_vector, anime_vector, 2)

    x = dot_user_anime + user_bias + anime_bias

    return tf.nn.sigmoid(x)

# inisialisasi model
model = RecommenderNet(total_user, total_anime, 50)

# Buat callback EarlyStopping
early_stopping = EarlyStopping(
    monitor='val_root_mean_squared_error',
    mode='min',
    patience=50,
    min_delta=0.001
)

model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = keras.optimizers.Adam(learning_rate=0.0001),
    metrics=[
        tf.keras.metrics.RootMeanSquaredError(),
    ],
)

history = model.fit(
    x = x_train,
    y = y_train,
    epochs = 5,
    validation_data = (x_test, y_test),
    callbacks=[early_stopping],
)

user_id = dataset_rating.user_id.sample(1).iloc[0]
anime_rated_by_user = dataset_rating[dataset_rating.user_id == user_id]

anime_not_rated = dataset_anime[~dataset_anime['anime_id'].isin(anime_rated_by_user.anime_id.values)]['anime_id']
anime_not_rated = list(
    set(anime_not_rated)
    .intersection(set(animeid_to_animeid_encoded.keys()))
)

anime_not_rated = [[animeid_to_animeid_encoded.get(x)] for x in anime_not_rated]
user_encoder = user_to_user_encoded.get(user_id)
user_anime_array = np.hstack(
    ([[user_encoder]] * len(anime_not_rated), anime_not_rated)
)

ratings = model.predict(user_anime_array).flatten()

top_ratings_indices = ratings.argsort()[-10:][::-1]
recommended_anime_ids = [
    animeid_encoded_to_animeid.get(anime_not_rated[x][0]) for x in top_ratings_indices
]

print('Showing recommendations for users: {}'.format(user_id))
print('===' * 9)
print('Resto with high ratings from user')
print('----' * 8)

top_anime_user = (
    anime_rated_by_user.sort_values(
        by = 'rating',
        ascending=False
    )
    .head(5)
    .anime_id.values
)

anime_df_rows = dataset_anime[dataset_anime['anime_id'].isin(top_anime_user)]
for row in anime_df_rows.itertuples():
    print(row.name, ':', row.genre)

print('----' * 8)
print('Top 10 resto recommendation')
print('----' * 8)

recommended_anime = dataset_anime[dataset_anime['anime_id'].isin(recommended_anime_ids)]
for row in recommended_anime.itertuples():
    print(row.name, ':', row.genre)

print(ratings.argsort()[:10])
print(ratings[:10])

"""CONTENT BASE"""

dataset_anime_content_base = pd.read_csv('/content/gdrive/MyDrive/Kaggle/anime.csv')
dataset_anime_content_base.dropna()

dataset_anime_content_base['type_and_genre'] = dataset_anime_content_base['type'].str.cat(dataset_anime_content_base.genre,sep=' ')
dataset_anime_content_base['anime_id'] = dataset_anime_content_base['anime_id'].unique()
dataset_anime_content_base['type_and_genre'].fillna('', inplace=True)

tfidf_vectorizer = TfidfVectorizer()
tfidf_matrix = tfidf_vectorizer.fit_transform(dataset_anime_content_base.type_and_genre)

cosine_sim_anime = cosine_similarity(tfidf_matrix)

cosine_sim_anime_df = pd.DataFrame(cosine_sim_anime,columns=dataset_anime.name,index=dataset_anime.name)

"""
- Mengambil data dengan menggunakan argpartition untuk melakukan partisi secara tidak langsung sepanjang sumbu yang diberikan
- Dataframe diubah menjadi numpy
- Range(start, stop, step)"""

def anime_recommendations(anime_name, similarity_data=cosine_sim_anime_df, items=dataset_anime[['name', 'genre']], k=5):

    index = similarity_data.loc[:,anime_name].to_numpy().argpartition(
        range(-1, -k, -1))

    closest = similarity_data.columns[index[-1:-(k+2):-1]]

    closest = closest.drop(anime_name, errors='ignore')

    return pd.DataFrame(closest).merge(items).head(k)

dataset_anime[dataset_anime.name.str.contains('zoro')]

anime_recommendations("Lupin III: Lupin Ikka Seizoroi")

print(animeid_to_animeid_encoded)